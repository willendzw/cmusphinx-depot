<HTML>
<HEAD>
<TITLE> Sphinx-II User Guide </TITLE>
  <style type="text/css">
     pre { font-size: medium; background: #f0f8ff; padding: 2mm; border-style: ridge ; color: teal}
     code {font-size: medium; color: teal}
  </style>
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000" LINK="#0000FF" VLINK="#0000B0">
<!--  BGCOLOR="#FFFFFF" TEXT="#000000" LINK="#0000FF" VLINK="#0000B0" -->

<H1 align=center> Sphinx-II User Guide </H1>

<center>
<p>
<a href="http://www.speech.cs.cmu.edu/sphinx/">CMU Sphinx Group</a><p>
Original by Ravishankar Mosur (Ravi) (<a href="mailto:rkm@cs.cmu.edu">rkm@cs.cmu.edu</a>)<br>
Maintained by Ravi and Kevin A. Lenzo (<a href="mailto:lenzo@cs.cmu.edu">lenzo@cs.cmu.edu</a>)<p>
School of Computer Science<br>
Carnegie Mellon University<br>
<strong>Copyright (c) 1997-2005 Carnegie Mellon University.</strong>
<HR>
This document is not complete, but should be helpful during construction.<br>
<HR>
</center>


<br><br><br><br><br><br>


<H2><U>Introduction</U></H2>

Sphinx2 is a decoding engine for the Sphinx-II speech recognition system developed at
Carnegie Mellon University.  It can be used to build small, medium or large
vocabulary applications.  Its main features are:
<UL>
<LI> Continuous speech decoding (as opposed to isolated word recognition)
<LI> Speaker-independent (doesn't require the user to train the system)
<LI> Ability to provide a single best or several alternative recognitions
<LI> Semi-continuous and continuous density acoustic models
<LI> Bigram, trigram, or finite-state grammar language models
<LI> Forced alignment and allphone recognition modes
</UL>

<P>
Sphinx2 consists of a set of libraries that include core speech recognition functions as
well as auxiliary ones such as low-level audio capture.  The libraries are written in
C and have been compiled on several Unix platforms (Linux, DEC Alpha, Sun Sparc, HPs) and
Pentium/PentiumPro PCs running WindowsXP, WindowsNT or Windows95.  A number of demo applications
based on this recognition engine are also provided.

<P>
Several features specifically intended for developing real applications have been
included in Sphinx2.  For example, many aspects of the decoder can be reconfigured at
run time.  New language models can be loaded or switched dynamically.  Similarly, new
words and pronunciations can be added.  The audio input data can be automatically
logged to files for any future analysis.

<P>
The rest of this document is structured as follows:
<UL>
<LI><A HREF="#sec_Sphinx2_sw">Obtaining Sphinx2 Software</A>
<LI><A HREF="#sec_models">Models for Running Sphinx2 Applications</A>
<LI><A HREF="#sec_engine">The Recognition Engine</A>
<LI><A HREF="#sec_api">The Application Programming Interface</A>
  <UL>
  <LI><A HREF="#sec_ad">Low-Level Audio Access</A>
  <LI><A HREF="#sec_cont_ad">Continuous Listening and Silence Filtering</A>
  <LI><A HREF="#sec_fbs">Speech-to-text Decoding</A>
  <LI><A HREF="#sec_allphone_api">Allphone Decoding</A>
  </UL>
<LI><A HREF="#sec_demos">Application Examples</A>
<LI><A HREF="#sec_Sphinx2_compile">Compiling the Libraries and Demos</A>
<LI><A HREF="#sec_cmdline">Arguments Reference</A>
  <UL>
  <LI><A HREF="#sec_cmdline_searchconfig">Decoder Configuration</A>
  <LI><A HREF="#sec_cmdline_beam">Beam Widths</A>
  <LI><A HREF="#sec_cmdline_langwt">Language Weights/Penalties</A>
  <LI><A HREF="#sec_cmdline_outputspec">Output Specifications</A>
  <LI><A HREF="#sec_cmdline_list">Alphabetical List of Arguments</A>
  </UL>
<LI><A HREF="#sec_faq">Frequently Asked Questions</A>
  <UL>
  <LI><A HREF="#sec_tune">Decoder Tuning</A>
  <LI><A HREF="#sec_lmdump">Building LM Dump Files</A>
  <LI><A HREF="#sec_sendump">Building 8-Bit Senone Dump Files</A>
  <LI><A HREF="#sec_fsgfmt">Finite State Grammar File Format</A>
  </UL>
</UL>


<br><br><br><br><br><br>


<H2><A NAME="sec_Sphinx2_sw"><U>Obtaining Sphinx2 Software</U></A></H2>

The Sphinx2 software is available on 
<a href="http://www.sourceforge.net/projects/cmusphinx/">SourceForge</a>.
<br>
<i>Note: This section is under construction.</i>


<br><br><br><br><br><br>


<H2><A NAME="sec_models"><U>Models for Running Sphinx2 Applications</U></A></H2>

In order to run Sphinx2 applications, several model files or databases are needed.
They fall into three broad categories: the <em>pronunciation lexicon</em> or
<em>dictionary</em>, the <em>acoustic model</em>, and the <em>language model</em>.
Applications should configure the decoder with the appropriate pronunciation,
acoustic and language models through command-line style
<A HREF="#sec_cmdline">arguments</A> that are described later in this document.

<P>

The following is a very brief description of each model.  (For those unfamiliar
with these topics or with Sphinx, a somewhat longer description, and the
associated Sphinx terminology, is in the html documentation for the Sphinx3 decoder,
available from <a href="http://www.sourceforge.net/projects/cmusphinx/">CMU Sphinx
at SourceForge</a>.)


<br><br>

<H3>Pronunciation Lexicon</H3>

The decoder must be initialized with one pronunciation lexicon or
dictionary that defines all the words of interest to the application and the
phonemic pronunciation for each word.

<P>

The dictionary can be modified at run time.  New words and their pronunciations can
be added to the lexicon in between <em>utterances</em>.  (An <em>utterance</em> is
the unit of decoding; see Section <A HREF="#sec_engine">The Recognition Engine</A>
below.)  There is no mechanism for removing a word from the lexicon.

<P>

In addition to ordinary words, a set of <em>noise</em> or <em>filler</em> words can
be specified by the application, by placing them in a corresponding dictionary
(indicated by the <strong><code>-ndictfn</code></strong> argument).  Sphinx2 also
automatically added the silence word <strong><code>SIL</code></strong>, with the
silence phone as its pronunciation, to the set
of filler words.  A filler dictionary is not required, but the silence word is
always added.
The significance of filler words is that they can occur anywhere
in the utterance, transparent to the language model (see below).

<P>

Finally, Sphinx2 also adds the distinguished begin-sentence and end-sentence symbols,
<strong><code>&lt;s&gt;</code></strong> and <strong><code>&lt;/s&gt;</code></strong>,
to the vocabulary.  These words must be present in any N-gram language model
(see below).



<br><br>

<H3>Acoustic Model</H3>

The Sphinx2 decoder can use either <em>semi-continuous</em> or <em>continuous</em>
density acoustic models generated by the <em>Sphinx acoustic model trainer</em>.
The decoder first checks the <strong><code>-mdeffn</code></strong> argument for a
a Sphinx3-format continuous density <em>model definition</em> file.  If present,
continuous model decoding is assumed.  Otherwise, the decoder looks for
semi-continuous acoustic models.

<P>

The decoder has to be initialized with one acoustic model.  It is not possible
to load multiple models and switch between them dynamically at run time.

<P>

<strong>Note</strong>:  The current implementation does not include any speed
optimizations to the continuous density model evaluation functions.  Hence,
applications using continuous density models may be restricted to small
vocabulary or small model configurations if they are to run in real time.


<H4>Semi-continuous Models</H4>

A Sphinx2 format semi-continuous acoustic model is a set of several files:
<ul>
  <li>Means and variances of the Gaussian density codebooks
    (<strong><code>.vec</code></strong> and <strong><code>.var</code></strong>
    files in the directory specified by the <strong><code>-cbdir</code></strong>
    argument),
  </li>
  <li>senone mixture weights (<strong><code>.ccode</code></strong>,
    <strong><code>.d2code</code></strong>, <strong><code>.p3code</code></strong>,
    and <strong><code>.xcode</code></strong> files, in the directory specified
    by the <strong><code>-hmmdir</code></strong> argument),
  </li>
  <li>HMM topology transition matrices (<strong><code>.chmm</code></strong>
    files, in the directory specified by the
    <strong><code>-hmmdirlist</code></strong> argument).
  </li>
</ul>

<P>

<strong>Note:</strong> the Sphinx
acoustic model trainer actually generates Sphinx3 format acoustic models,
which consist of four files: <em>means</em>, <em>variances</em>, <em>mixture
weights</em> and <em>transition matrices</em>.  They have to be converted to
Sphinx2 format using utilities provided with the trainer.

<P>

The semi-continuous model files generated by the trainer have model parameter
values in 32-bit format.  Sphinx2 memory requirements can be reduced considerably
by combining all the mixture weights files into a single, compressed
<em>senone dump file</em> that uses 8-bit parameter values.  See Section
<A HREF="#sec_sendump">Building 8-Bit Senone Dump Files</A> for details.

<P>

Given the acoustic model and a pronunciation dictionary,
there is also an associated mapping information that defines the <em>senone
mapping</em> for each <em>triphone state</em> encountered in the dictionary.
For semi-continuous models, this information is available in
<strong><code>.phone</code></strong> and <strong><code>.map</code></strong> files
(specified by <strong><code>-phnfn</code></strong> and
<strong><code>-mapfn</code></strong> arguments).  These two files are also
generated by utilities in the Sphinx trainer package, based on a corresponding
Sphinx3-format <em>model definition</em> file.



<H4><A NAME="sec_acmod_cont">Continuous Density Models</A></H4>

Sphinx2 directly uses most of the Sphinx3 format continuous density acoustic
model files without requiring any format conversion.  As mentioned above, if the
<strong><code>-mdeffn</code></strong> argument is specified, continuous density
models are assumed.  This argument specifies a Sphinx3 format <em>model
definition</em> file, containing triphone-state to senone mapping information.
There is no need to convert this file into <strong><code>.phone</code></strong>
and <strong><code>.map</code></strong> files, unlike in the case of
semi-continuous models.

<P>

However, in the current implementation, the decoder does internally generate
<strong><code>.phone</code></strong> and <strong><code>.map</code></strong>
files from the model definition file.  It writes them to a directory specified
by the <strong><code>-kbdumpdir</code></strong> argument.  Applications may
save the generated files, and specify them as input in subsequent decoder runs.

<P>

To summarize, if the <strong><code>-mdeffn</code></strong> argument is specified,
the decoder assumes continuous density models.  
If the <strong><code>-phnfn</code></strong> and <strong><code>-mapfn</code></strong>
arguments are not specified, the decoder automatically generates
<strong><code>.phone</code></strong> and <strong><code>.map</code></strong> files
in the <strong><code>-kbdumpdir</code></strong> directory, and reads them back in.
If the <strong><code>-phnfn</code></strong> and <strong><code>-mapfn</code></strong>
are specified, on the other hand, the decoder does not generate new
<strong><code>.phone</code></strong> and <strong><code>.map</code></strong>
files.  Instead, it simply reads in the files specified by these arguments, with
the assumption that they are compatible with the model definition file.

<P>

As for the continuous density acoustic models themselves, the decoder can directly
read in the <em>means</em>, <em>variances</em> and
<em>mixture weights</em> files generated by the Sphinx3 trainer, without any
format conversion.  The <em>transition matrices</em> file does, however, have to
be converted to Sphinx2 format (<strong><code>.chmm</code></strong> files).

<P>

There are a few restrictions on the structure of continuous density acoustic
models that the Sphinx2 decoder can handle:
<ul>
<li> The model topology is restricted to the 5-state, Bakis topology used by
Sphinx2.
<li> The <em>acoustic feature vector type</em> is restricted to a single-stream,
39-element vector, consisting of 12 cepstra, 12 delta-cepstra, 3 power, and 12
delta-delta-cepstra concatenated together (i.e., the
<strong><code>1s_12c_12d_3p_12dd</code></strong> feature type in the Sphinx3
trainer).
</ul>



<br><br>

<H3>Language Model</H3>

Sphinx2 accepts two flavors of language models (LMs) or grammars:
<em>finite-state</em> (FSG), and <em>N-gram</em> (for <em>N</em> = 2 or 3, i.e.,
bigrams or trigrams).  One can load
multiple LMs, N-gram or FSG, into the decoder, either during
initialization or at run time.
However, only one LM can be active for a given utterance.  LMs are identified by
a <em>string name</em>.  The application can switch LMs in between utterances.
The N-gram language model specified by the <strong><code>-lmfn</code></strong>
argument is <em>unnamed</em>; it has the empty string as its name.

<P>

The <em>active vocabulary</em> during the decoding of an utterance is the set of
words that is present in both the pronunciation dictionary and the currently
active LM.  The decoder is incapable of recognizing any word outside the active
vocabulary.

<P>

Any new word added to the dictionary is also automatically added as a unigram to
the <em>unnamed</em> N-gram language model.  (<em>This is a HACK</em>.
There ought to be a mechanism for adding a word to a specified language model.
Currently the only way to accomplish this is to delete a currently loaded language
model, create a new model with the new word, and load it.  This is also the case
with FSGs; there is no way to dynamically modify a loaded FSG.)

<P>

As mentioned earlier, an N-gram language model must include the begin-sentence
and end-sentence symbols, <strong><code>&lt;s&gt;</code></strong> and
<strong><code>&lt;/s&gt;</code></strong>.
Filler words are transparent to any language model,
N-gram or finite state.  That is, the decoder can transparently try to insert
them anywhere in the utterance, but they don't exist as far as the language model
is concerned.

<P>

Large N-gram LMs load very slowly.  The delay can be avoided by
providing a <em>binary dump</em> version of LM files along with the original
LMs.  The Sphinx2 decoder can automatically create LM dump
files for large N-gram LMs, which can be used in subsequent decoder runs.  (See
Section <A HREF="#sec_lmdump">Building LM Dump Files</A>.)

<P>

Note that the current
implementation of finite-state grammars, or FSGs, is not the most efficient.
In particular, transitions are represented using a full <em>N</em>x<em>N</em>
matrix, where <em>N</em> is the number of states.  Hence,
FSGs containing several thousands of states may run inefficiently.

<P>

No LM is required for operation in allphone or forced alignment
recognition modes.

<P>


<br><br><br><br><br><br>


<H2><A NAME="sec_engine"><U>The Recognition Engine</U></A></H2>

The core speech decoder operates on finite-length segments of speech or
<em>utterances</em>, one utterance at a time.
(Operationally, an utterance is the chunk of speech data processed
between calls to the Sphinx2 API functions
<strong><code>uttproc_begin_utt</code></strong> and the next
<strong><code>uttproc_end_utt</code></strong>; see further below.)
An utterance can be up to a minute long.  In practice, most applications would
typically treat a sentence or a phrase as an utterance, which would be much
shorter than the maximum of 60 sec.


<br><br>

<H3>Basic Recognition</H3>

The recognition structure in Sphinx2 depends on whether the currently active
language model is an N-gram or an FSG model.  (The API, however, is substantially
the same for both.)


<H4>N-gram Decoding</H4>

In N-gram decoding, each utterance is decoded using up to three <em>passes</em>,
two of which are optional:
<UL>
<LI> A <em>lexical-tree Viterbi</em> search.  It produces a recognition result
as well as a <em>word lattice</em>.
<LI> An optional <em>flat-structured</em> Viterbi search restricted to the word
lattice from above.  It produces a new recognition result and a new word lattice,
replacing those from the first pass.
<LI> An optional <em>global best-path</em> search of the word lattice, producing
yet another recognition result.
</UL>
The optional passes generally improve recognition accuracy.  However, the second
pass (flat Viterbi search) can increase latency significantly.
The passes to be active are configured once at initialization.  From then on, the
presence of the multiple passes is invisible to the application.  It only receives
the result from the last active pass.  However, the word lattice can subsequently
be searched for additional, alternative--or <em>N-best</em>--hypotheses by the
application.


<H4> FSG Decoding</H4>

In FSG decoding, on the other hand, there is only one pass of
Viterbi search.  The optional passes described above are not used, and neither is
it possible to obtain N-best lists (yet).  (Moreover, implementation of even the
one pass is a distinct from the N-gram decoder.  This is in order to facilitate
porting FSG decoding to Sphinx3.)

<P>

Details of the Sphinx2 recognition engine (with N-gram models) can be found in
<A HREF="http://reports-archive.adm.cs.cmu.edu/anon/1996/CMU-CS-96-143.ps">Ravishankar's Ph.D
thesis</A>.


<br><br>

<H3>Forced Alignment and Allphone Recognition Modes</H3>

The recognizer can be run to <em>time-align</em> given transcripts to input
speech, producing time segmentations for the input transcripts, as well as identifying
silence regions.  Time-alignment is only available in batch mode.  It is covered in
more detail below.  (<em>Note:</em>  Forced alignment can also be accomplished
using the recently added finite-state grammar capability, which can also be used in
live mode.  However, it cannot provide phoneme and state-level segmentation.)

<P></P>

Sphinx2 can also be used in <em>allphone</em> mode to produce a purely phonetic
recognition instead of the normal word recognition.  The allphone recognition API is
available to user-written applications as well.  However, the input can only be from
pre-recorded files.
<P></P>

<strong>Note:</strong> The recognition engine is configured in one of <em>normal</em>,
<em>forced-alignment</em>, or <em>allphone</em> modes during initialization.  It
cannot be dynamically switched between these modes at run time.



<br><br><br><br><br><br>


<H2><A NAME="sec_api"><U>The Application Programming Interface</U></A></H2>

There are three main groups of functions or application programming interface
(API) available with Sphinx2: raw audio access, continuous listening/silence
filtering, and the core decoder itself.

<P>
As we shall see below, none of the core decoder API functions directly accesses
any audio device.  Rather, the application is responsible for collecting
audio data to be decoded.  This gives applications the freedom to decode audio
data originating at any source at all---standard audio devices, pre-recorded files,
data received from a remote location over a socket connection, etc.
Since most applications
ultimately need to access common audio devices and to perform some form of silence
filtering to detect speech/no-speech conditions, the two additional modules are
provided as a convenience.

<P>

(<strong>NOTE:</strong> The APIs often use <strong><code>int32</code></strong>
and <strong><code>int16</code></strong> types, which are basically 32-bit and
16-bit integer types.  Similarly, <strong><code>uint32</code></strong> and
<strong><code>uint16</code></strong> are the unsigned versions.)



<br><br>

<H3><A NAME="sec_ad">Low-Level Audio Access</A></H3>

No two platforms provide the same interface to audio devices.  To accommodate this
diversity, the platform-dependent code is encapsulated within a generic interface
for low-level audio recording and playback.  The following functions are for recording.
Complete details can be found in
<strong><code>include/ad.h</code></strong>.
<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_open:</code></strong>
</TD>
<TD VALIGN=TOP>
Opens an audio device for recording.  Returns a <em>handle</em>
to the opened device.  (Currently 8KHz or 16KHz mono, 16-bit PCM only.)
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_start_rec:</code></strong>
</TD>
<TD VALIGN=TOP>
Starts recording on the audio device associated with the specified handle.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_read:</code></strong>
</TD>
<TD VALIGN=TOP>
Reads up to a specified number of samples into a given buffer.  It returns the number
of samples actually read, which may be less than the number requested.  In particular
it may return 0 samples if no data is available.  Most operating systems have a limited
amount of internal buffering (at most a few seconds) for audio devices.  Hence, this
function must be called frequently enough to avoid buffer overflow.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_stop_rec:</code></strong>
</TD>
<TD VALIGN=TOP>
Stops recording.  (However, the system may still have internally buffered data
remaining to be read.)
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_close:</code></strong>
</TD>
<TD VALIGN=TOP>
Closes the audio device associated with the specified audio handle.
</TD>
</TR>

</TABLE>

See <strong><code>examples/adrec.c</code></strong>
and <strong><code>examples/adpow.c</code></strong>
for two examples demonstrating the use of the above functions.

<P>



A similar set of playback functions are provided (<em>currently implemented only on
PC/Windows platforms</em>):

<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_open_play:</code></strong>
</TD>
<TD VALIGN=TOP>
Opens an audio device for playback.  Returns a <em>handle</em>
to the opened device.  (Currently 8KHz or 16KHz mono, 16-bit PCM only.)
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_start_play:</code></strong>
</TD>
<TD VALIGN=TOP>
Starts playback on the device associated with the given handle.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_write:</code></strong>
</TD>
<TD VALIGN=TOP>
Sends a buffer of samples for playback.  The function may
accept fewer than the samples provided, depending on available internal buffers.
It returns the number of samples actually accepted.  The application must provide data
sufficiently rapidly to avoid breaks in playback.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_stop_play:</code></strong>
</TD>
<TD VALIGN=TOP>
End of playback.  Playback is continued until all buffered data has been consumed.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>ad_close_play:</code></strong>
</TD>
<TD VALIGN=TOP>
Closes the audio device associated with the specified handle.
</TD>
</TR>

</TABLE>

Finally, the audio library includes a function
<strong><code>ad_mu2li</code></strong> for converting 8-bit mu-law samples into
16-bit linear PCM samples.
<P>

See <strong><code>examples/adplay.c</code></strong>
for an example that plays back audio samples from a given input file.

<P>

The implementation of the audio API for various platforms is contained in
analog-to-digital library for the given architecture.


<br><br>

<H3><A NAME="sec_cont_ad">Continuous Listening and Silence Filtering</A></H3>

As mentioned earlier, Sphinx2 can only decode utterances that are limited to less than
about 1 min. at a time.  However, one often wants to leave the audio recording
running continuously and automatically determine utterance boundaries based on
pauses in the input speech.  The continuous listening module in Sphinx2 provides the
mechanisms for this purpose.

<P>
The silence filtering module is interposed between the raw audio input source
and the application.  The application calls the function
<strong><code>cont_ad_read</code></strong> instead of directly reading the raw A/D
input source.
<strong><code>cont_ad_read</code></strong> returns only those segments of
input audio that it determines to be non-silence.  Additional timestamp information
is provided to inform the application about silence regions that have been dropped.


<P>

The complete continuous listening API is defined in
<strong><code>include/cont_ad.h</code></strong></A>
and is summarized below:

<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_init:</code></strong>
</TD>
<TD VALIGN=TOP>
Associates a new continuous listening module instance with a specified raw A/D
<em>handle</em> and a corresponding <em>read</em> function pointer.  E.g., these
may be the handle returned by <strong><code>ad_open</code></strong> and function
<strong><code>ad_read</code></strong> described <A HREF="#sec_ad">above</A>.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_calib:</code></strong>
</TD>
<TD VALIGN=TOP>
Calibrates the background silence level by reading the raw audio for a few seconds.
It should be done once immediately after <strong><code>cont_ad_init</code></strong>,
and after any environmental change.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_read:</code></strong>
</TD>
<TD VALIGN=TOP>
Reads and returns the next available block of non-silence data in a given buffer.
(Uses the <em>read</em> function and <em>handle</em> supplied to
<strong><code>cont_ad_init</code></strong> to obtain the raw A/D data.)  More details
are provided below.

</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_reset:</code></strong>
</TD>
<TD VALIGN=TOP>
Flushes any data buffered inside the module.  Useful for discarding accumulated,
but unprocessed speech.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_get_params:</code></strong>
</TD>
<TD VALIGN=TOP>
Returns the current values of a number of parameters that determine the functioning
of the silence/speech detection module.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_set_params:</code></strong>
</TD>
<TD VALIGN=TOP>
Sets a number of parameters that determine the functioning of the silence/speech
detection module.  Useful for fine-tuning its performance.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_set_thresh:</code></strong>
</TD>
<TD VALIGN=TOP>
Useful for adjusting the silence and speech thresholds.  (It's preferable to use
<strong><code>cont_ad_set_params</code></strong> for this purpose.)
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_detach:</code></strong>
</TD>
<TD VALIGN=TOP>
Detaches the specified continuous listening module from its currently associated
audio device.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_attach:</code></strong>
</TD>
<TD VALIGN=TOP>
Attaches the specified continuous listening module to the specified audio device.
(Similar to <strong><code>cont_ad_init</code></strong>, but without the need to
calibrate the audio device.  The existing parameter values are used instead of
being reset to default values.)
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>cont_ad_close:</code></strong>
</TD>
<TD VALIGN=TOP>
Closes the continuous listening module.
</TD>
</TR>

</TABLE>

Some additional details on the <strong><code>cont_ad_read</code></strong> function
are in order.
Operationally, every call to <strong><code>cont_ad_read</code></strong> causes the
module to read the associated raw A/D source (as much data as possible and available),
scan it for speech (non-silence) segments and enqueue them internally.  It returns
the first available segment of speech data, if any.  In addition to returning
non-silence data, the function also updates a couple of parameters that may be of
interest to the application:
<UL>
<li> The <em>signal level</em> for the most recently read data.  This is the
    <strong><code>siglvl</code></strong> member variable of the
    <strong><code>cont_ad_t</code></strong> structure returned by
    <strong><code>cont_ad_init()</code></strong>.
</li>
<li> A <em>timestamp</em> value indicating the total number of raw audio samples
    that have been consumed at the end of the most recent
    <strong><code>cont_ad_read()</code></strong> call.  This is in
    the <strong><code>read_ts</code></strong> member variable of the
    <strong><code>cont_ad_t</code></strong> structure.
</li>
</UL>
So, for example, if on two successive calls to
<strong><code>cont_ad_read</code></strong>, the timestamp is 100000 and 116000,
respectively, the application can determine that 1 sec (16000 samples) of silence
have been gobbled up between the two calls.
<P>

Silence regions aren't chopped off completely.  About 50-100ms worth of silence
is preserved at either end of a speech segment and passed on to the application.
<P>

Finally, the continuous listener won't concatenate speech segments separated by
silence.  That is, the data returned by a single call to
<strong><code>cont_ad_read</code></strong> will not span raw audio separated by
silence that has been gobbled up.
<P>

<strong><code>cont_ad_read</code></strong> must be called frequently enough to avoid
loss of input data owing to buffer overflow.  The application is responsible for
turning actual recording on and off, if applicable.  In particular, it must ensure
that recording is on during calibration and normal operation.

<P>

See <strong><code>examples/cont_adseg.c</code></strong>
for an example that uses the continuous listening module to segment live audio input
into separate utterances.  Similarly,
<strong><code>examples/cont_fileseg.c</code></strong>
segments a given pre-recorded file containing audio data into utterances.


<br><br>

<H3><A NAME="sec_fbs">Speech-to-Text Decoding</A></H3>

There are several aspects to speech decoding: initialization, basic speech
decoding, management of multiple grammars (LMs), logging and book-keeping, etc.
This section briefly describes the related Sphinx2 API functions.  Note that not
all the available functions are documented here.  Please refer to the functions
and data types defined in <strong><code>include/fbs.h</code></strong> for further
details.

<P>

The two functions pertaining to initialization and final cleanup are:
<TABLE>
<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>fbs_init:</code></strong>
</TD>
<TD VALIGN=TOP>
Initializes the decoder.  The input arguments (in the form of the common
command line argument list <strong><code>argc,argv</code></strong>) specify the
input databases (acoustic, lexical, and language models) and various other
decoder configuration options.  (See <A HREF="#sec_cmdline">Arguments Reference</A>.)
If batch-mode processing is indicated
(see <A HREF="#sec_cmdline_searchconfig"><strong><code>-ctlfn</code></strong></A>
option below) it happens as part of this initialization.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>fbs_end:</code></strong>
</TD>
<TD VALIGN=TOP>
Cleans up the internals of the decoder, such as printing summaries and closing log
files, before the application exits.
</TD>
</TR>

</TABLE>

<P><br>



Sphinx2 applications can use the following functions to decode speech into text,
one utterance at a time:
<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_begin_utt:</code></strong>
</TD>
<TD VALIGN=TOP>
Begins decoding the next utterance.  The application can assign an <em>id</em>
string to it.  If not, one is automatically created and assigned.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_rawdata:</code></strong>
</TD>
<TD VALIGN=TOP>
Processes (decodes) the next chunk of raw A/D data in the current utterance.  This
can be <em>non-blocking</em>, in which case much of the data may be simply queued
internally for later processing.  Note that only single-channel (mono) 16-bit linear
PCM-encoded samples can be processed.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_cepdata:</code></strong>
</TD>
<TD VALIGN=TOP>
This is an alternative to <strong><code>uttproc_rawdata</code></strong> if the
application wishes to decode <em>cepstrum</em> data instead of raw A/D data.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_end_utt:</code></strong>
</TD>
<TD VALIGN=TOP>
Indicates that all the speech data for the current utterance has been provided to
the decoder.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_result:</code></strong>
</TD>
<TD VALIGN=TOP>
Finishes processing internally queued up data and returns the final recognition
result string.  It can also be <em>non-blocking</em>, in which case it may return
after processing only some of the internally queued up data.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_result_seg:</code></strong>
</TD>
<TD VALIGN=TOP>
Like <strong><code>uttproc_result</code></strong>, but returns additional information
for each word in the result, such as time segmentation (measured in 10msec frames),
acoustic and language model scores, etc.  (See structure <strong><code>search_hyp_t</code></strong>
in file <strong><code>include/fbs.h</code></strong>.)
One can use either this function or <strong><code>uttproc_result</code></strong> to
finish decoding, but not both.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_partial_result:</code></strong>
</TD>
<TD VALIGN=TOP>
This function can be used to obtain the
most up-to-date partial result while utterance decoding is in progress.  This may be
useful, for example, in providing feedback to the user.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_partial_result_seg:</code></strong>
</TD>
<TD VALIGN=TOP>
Like <strong><code>uttproc_partial_result</code></strong>, but returns word
segmentation information (measured in 10msec frames) instead of the recognition
string.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_abort_utt:</code></strong>
</TD>
<TD VALIGN=TOP>
This is an alternative to <strong><code>uttproc_end_utt</code></strong> that
terminates the current utterance.  No further recognition results can be obtained for it.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>search_get_alt:</code></strong>
</TD>
<TD VALIGN=TOP>
Returns N-best hypotheses for the utterance.  Currently, this does not work with
finite state grammars.
(See further details in <strong><code>include/fbs.h</code></strong>).
</TD>
</TR>

</TABLE>
The <em>non-blocking</em> option in some of the above functions is useful if
decoding is slower than real-time, and there is a chance of losing input A/D
data if processing them takes too long.  In the non-blocking mode, the data may
simply be queued up internally and processed only after all the input data for
the current utterance has been acquired.  Similarly, the non-blocking option in
<strong><code>uttproc_result</code></strong> allows the application to respond
to user-interface events in real-time.

<P>

The application code fragment for decoding one utterance typically looks as follows:
<pre>
    uttproc_begin_utt (....)
    while (not end of utterance) {   /* indicated externally, somehow */
	read any available A/D data; /* possibly 0 length */
        uttproc_rawdata (A/D data read above, non-blocking);
    }
    uttproc_end_utt ();
    uttproc_result (...., blocking);
</pre>
See several demo applications in the directory
<strong><code>examples/</code></strong> for some variations.

<P>


Multiple, named LMs can be resident with the decoder module, either
read in during initialization, or dynamically at run time.  However, exactly
one LM must be selected and active for decoding any given utterance.
As mentioned earlier, the active vocabulary for each utterance is given by the
intersection of the pronunciation dictionary and the currently active
LM.  The following functions allow the application to control language
modelling related aspects of the decoder:
<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>lm_read:</code></strong>
</TD>
<TD VALIGN=TOP>
Reads in a new N-gram language model from a given file,
and associates it with a given string name.  The application needs this function
only if it needs to create and load LMs dynamically at run time, rather than
at initialization via the <strong><code>-lmfn</code></strong> command line
argument.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>lm_delete:</code></strong>
</TD>
<TD VALIGN=TOP>
Deletes the N-gram LM with the given string name from the decoder
repertory.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_set_lm:</code></strong>
</TD>
<TD VALIGN=TOP>
Tells the decoder to switch the active grammar to the N-gram LM with
the given string name.  Subsequent utterances are decoded with this grammar,
until the next <strong><code>uttproc_set_lm</code></strong> or
<strong><code>uttproc_set_fsg</code></strong> operation.
This function can only be invoked between utterances, not in the midst of one.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_set_context:</code></strong>
</TD>
<TD VALIGN=TOP>
Sets a two-word history for the next utterance to be decoded, giving its first
words additional context that can be exploited by the LM.  (Useful only with
N-gram LMs.)
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_load_fsgfile:</code></strong>
</TD>
<TD VALIGN=TOP>
Loads the given finite-state grammar (FSG) file into the system and returns the
string name associated with the FSG.  (Unlike the N-gram LM, the string name
is contained in the FSG file.)  The application needs this function only
if it needs to create and load FSGs dynamically at run time, rather than
at initialization via the <strong><code>-fsgfn</code></strong> or
<strong><code>-fsgctlfn</code></strong> command line arguments.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_load_fsg:</code></strong>
</TD>
<TD VALIGN=TOP>
Similar to <strong><code>uttproc_load_fsgfile</code></strong>, but the input
FSG is provided in the form of an <strong><code>s2_fsg_t</code></strong> data
structure (see <strong><code>include/fbs.h</code></strong>), instead of a file.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_set_fsg:</code></strong>
</TD>
<TD VALIGN=TOP>
Tells the decoder to switch the active grammar to the FSG with
the given string name.  Subsequent utterances are decoded with this grammar,
until the next <strong><code>uttproc_set_fsg</code></strong> or
<strong><code>uttproc_set_lm</code></strong> operation.
This function can only be invoked between utterances, not in the midst of one.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_del_fsg:</code></strong>
</TD>
<TD VALIGN=TOP>
Deletes the FSG with the given string name from the decoder repertory.
</TD>
</TR>

</TABLE>

<P><br>



The raw input data for each utterance and/or the cepstrum data derived from it
can be logged to specified directories:
<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_set_rawlogdir:</code></strong>
</TD>
<TD VALIGN=TOP>
Specifies the directory to which utterance audio data should be logged.  An
utterance is logged to file &lt;id&gt;.raw, where &lt;id&gt; is the string
ID assigned to utterance by <strong><code>uttproc_begin_utt</code></strong>.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_set_mfclogdir:</code></strong>
</TD>
<TD VALIGN=TOP>
Specifies the directory to which utterance cepstrum data should be logged.
Like A/D files above, an utterance is logged to file &lt;id&gt;.mfc.
</TD>
</TR>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_get_uttid:</code></strong>
</TD>
<TD VALIGN=TOP>
Retrieves the utterance ID string for the current or most recent utterance.
Useful for locating the logged A/D data and cepstrum files, for example.
</TD>
</TR>

</TABLE>

<P>

In addition, the decoder configuration includes a number of parameters that can
be tuned for a given application to give optimum performance.  They
are set at initialization time via command-line style arguments (during the
<strong><code>fbs_init</code></strong> call.  The parameters determine
various aspects of the decoder, such as <em>beam pruning thresholds</em>,
the relative weights of acoustic and language model scores, etc.  They are
covered in more detail in Section <A HREF="#sec_cmdline">Arguments Reference</A>
below.


<br><br>

<H3><A NAME="sec_allphone_api">Allphone Decoding</A></H3>

In batch mode, Sphinx2 runs in allphone mode if the <strong><code>-allphone</code></strong>
flag is <b><code>TRUE</code></b>.  In this mode, no
language model should be provided; i.e., the <strong><code>-lmfn</code></strong>,
<strong><code>-lmctlfn</code></strong>, <strong><code>-fsgfn</code></strong> and
<strong><code>-fsgctlfn</code></strong> arguments should be omitted.  A phone
transition probability matrix can be specified using the
<strong><code>-phonetpfn</code></strong> argument.

<P>

In addition, the API for allphone decoding includes a single function that supports
recognition from pre-recorded files:
<TABLE>

<TR>
<TD nowrap VALIGN=TOP>
<LI><strong><code>uttproc_allphone_file:</code></strong>
</TD>
<TD VALIGN=TOP>
Performs allphone recognition on the given file and returns the resulting phone
segmentation.  The input file can contain either audio data, or cepstrum data.
The <strong><code>-adcin</code></strong> argument, and any related ones, should
be set accordingly.  (See <A HREF="#sec_cmdline">arguments reference</A> below.)
</TD>
</TR>

</TABLE>


<br><br>

<H3>Forced Alignment</H3>

Sphinx2 (in batch mode) can be used for aligning transcripts to speech, in order to
obtain time-segmentations at the word, phone, or state levels.  In this mode, no
language model should be provided; i.e., the <strong><code>-lmfn</code></strong>,
<strong><code>-lmctlfn</code></strong>, <strong><code>-fsgfn</code></strong> and
<strong><code>-fsgctlfn</code></strong> arguments should be omitted.

<P>

The set of utterances (speech data) is given by
the <strong><code>-ctlfn</code></strong> argument, as usual.
(See <A HREF="#sec_cmdline">arguments reference</A> below.)  In addition,
the corresponding transcripts should be given
in a parallel file, which should be the <strong><code>-tactlfn</code></strong>
argument.  The first line of this file 
should contain just the string <strong><code>*align_all*</code></strong>.
This should be followed by the transcripts for all the utterances to be
aligned, one line per utterance, in the same order as in
the <strong><code>-ctlfn</code></strong> file.
The transcripts must not include any utterance ID.

<P>

Alignments at the word, phone and state levels can be obtained by setting the flags
<strong><code>-taword</code></strong>, <strong><code>-taphone</code></strong>, and
<strong><code>-tastate</code></strong> individually to
<strong><code>TRUE</code></strong> or <strong><code>FALSE</code></strong>.  Alignments
are written to stdout (the log file).



<br><br><br><br><br><br>


<H2><A NAME="sec_demos"><U>Application Examples</U></A></H2>

Two simple speech decoding applications, implemented with a tty-based interface as
well as with a Windows interface, are included in directory
<strong><code>examples:</code></strong>
<UL>
<LI><strong><code>sphinx2_ptt:</code></strong>
demonstrates an application in which the user explicitly indicates the start and
end of each utterance using the &lt;<code>RETURN</code>&gt; keyboard key.
(On WindowsNT/Windows95 systems, the ending &lt;<code>RETURN</code>&gt; is not
used.  Instead, the utterance is terminated after a fixed duration.)
<p>
<LI><strong><code>sphinx2_continuous:</code></strong>
demonstrates the interaction of continuous listening and decoding.  An endless
audio input stream is automatically segmented into utterances using the continuous
listening module, and the utterances are decoded.  The timestamps returned by the
continuous listening module are used to locate gaps in speech data of at least 1 sec,
thus marking the utterance boundaries.
</UL>

<P>


<br><br><br><br><br><br>


<H2><A NAME="sec_Sphinx2_compile"><U>Compiling the Libraries and Demos</U></A></H2>

To compile Sphinx2 libraries on Unix platforms:
<UL>
<LI> Unpack the distribution
<LI> Run <strong><code>sh autogen.sh</code></strong> if necessary (not necessary if you are using the Sphinx2 release package)
<LI> Run <strong><code>./configure</code></strong>
<LI> Alternatively, run <strong><code>./configure --prefix=[<em>path_to_install_dir</em>]</code></strong> if you do not want to install Sphinx2 in the default location
<LI> <strong><code>make</code></strong>
<LI> <strong><code>make test</code></strong>
<LI> <strong><code>make install</code></strong>
</UL>

<P>

To compile Sphinx2 libraries on Windows platforms:
<UL>
<LI> Unpack the distribution
<LI> Load the workspace <strong><code>.\sphinx2.dsw</code></strong> onto Microsoft Visual C++ 6.0 or better.
<LI> Compile the project <code><strong>sphinx2_continuous</code></strong>, minimally
<li> On a command prompt (follow the menu "<code>Start</code>" -> "<code>Run</code>" and type  <code><strong>cmd</strong></code>), cd
to the location where you installed sphinx2 (e.g., <code><strong>c:\sphinx2-0.5</strong></code>) 
<LI> Still in the command prompt, <code><strong>cd .\win32\batch</code></strong>.
<LI> Run <code><strong>sphinx2-test.bat</code></strong>
</UL>

<p>MS Visual Studio will build the executables under
<code><strong>.\bin\Release</strong></code> or
<code><strong>.\bin\Debug</strong></code> (depending on the setting
you choose on MS Visual Studio), and the libraries under
<code><strong>.\lib\Release</strong></code> or
<code><strong>.\lib\Build</strong></code>.
</p>

<p>In a successful installation, the test produces the recognition result <code><strong>GO FORWARD TEN METERS</code></strong></p>
<br><br><br><br><br><br>

<H2><A NAME="sec_cmdline"><U>Arguments Reference</U></A></H2>

The core Sphinx2 decoding engine accepts a long list of arguments during
initialization.  These are the arguments to the library function
<strong><code>fbs_init(int argc, char *argv[])</code></strong> defined in
<strong><code>include/fbs.h</code></strong>.
(Applications built around the Sphinx2 libraries, of course, can have additional
arguments.)  Many arguments, such as the input model databases, must be
specified by the user.  We cover the more important ones below (the remaining
have reasonable default values):



<br><br>

<H3><A NAME="sec_cmdline_inputdb">Input Model Databases</A></H3>

<TABLE border>

<TR>
<TH>Flag</TH>
<TH>Description</TH>
<TH>Default</TH>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-lmfn</code></strong>
</TD>
<TD VALIGN=TOP>
Optional DARPA format N-gram LM file with the empty string as its name.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-lmctlfn</code></strong>
</TD>
<TD VALIGN=TOP>
Optional control file with a list of N-gram LM files and associated string names
(one line per entry).  This is how multiple LMs can be loaded during initialization.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-kbdumpdir</code></strong>
</TD>
<TD VALIGN=TOP>
Optional directory containing precompiled binary versions of N-gram LM files
(see <A HREF="#sec_lmdump">Building LM Dump Files</A>).  Also, directory in
which Sphinx2 format <a href="#sec_acmod_cont">map and phone files</a> are
created if <strong><code>-mdeffn</code></strong> is specified but
<strong><code>-phnfn</code></strong> and <strong><code>-mapfn</code></strong>
are omitted.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fsgfn</code></strong>
</TD>
<TD VALIGN=TOP>
Optional finite state grammar file.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fsgctlfn</code></strong>
</TD>
<TD VALIGN=TOP>
Optional control file with a list of FSG grammar filenames.  One line per entry.  Blank
and comment lines (beginning with a <strong><code>#</code></strong> char) are allowed.
This is how multiple FSGs can be loaded during initialization.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fsgusealtpron</code></strong>
</TD>
<TD VALIGN=TOP>
Whether the decoder should expand a transition in an FSG to include all the alternative
pronunciations of the associated word.
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fsgusefiller</code></strong>
</TD>
<TD VALIGN=TOP>
Whether the decoder should insert a filler word (self-loop) transition automatically
at every state.
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-dictfn</code></strong>
</TD>
<TD VALIGN=TOP>
Main pronunciation dictionary file.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-oovdictfn</code></strong>
</TD>
<TD VALIGN=TOP>
Optional out-of-vocabulary (OOV) pronunciation dictionary.  These are added to the
unnamed LM (read from <strong><code>-lmfn</code></strong> file) with unigram
probability given by <strong><code>-oovugprob</code></strong>.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-ndictfn</code></strong>
</TD>
<TD VALIGN=TOP>
Optional "noise" words pronunciation dictionary.  Noise words are not part of any
LM and, like silence, can be inserted transparently anywhere in the utterance.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-phnfn</code></strong><br>
<strong><code>-mapfn</code></strong>
</TD>
<TD VALIGN=TOP>
Phone and map files with senone mapping information for the given dictionary and
acoustic model.  Can be omitted for continuous models (i.e., if
<strong><code>-mdeffn</code></strong> is specified).
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-cbdir</code></strong>
</TD>
<TD VALIGN=TOP>
Directory containing Sphinx-2 format semi-continuous acoustic model codebook
files (<strong><code>.vec</code></strong> and <strong><code>.var</code></strong>
files).
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-hmmdir</code></strong>
</TD>
<TD VALIGN=TOP>
Directory containing Sphinx-2 format semi-continuous acoustic model senone
weights files (<strong><code>.ccode</code></strong>,
<strong><code>.d2code</code></strong>, <strong><code>.p3code</code></strong>,
and <strong><code>.xcode</code></strong> files).
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-hmmdirlist</code></strong>
</TD>
<TD VALIGN=TOP>
Directory containing Sphinx2-format acoustic model transition matrices files
(<strong><code>.chmm</code></strong> files).  (For both semi-continuous and
continuous density models.)
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-sendumpfn</code></strong><br>
<strong><code>-8bsen</code></strong>
</TD>
<TD VALIGN=TOP>
Optional 8-bit senone mixture weights file created from the 32-bit mixture weights
files (see <A HREF="#sec_sendump">Building 8-Bit Senone Dump Files</A>).
<strong><code>-8bsen</code></strong> should be <strong><code>TRUE</code></strong> if
the 8-bit senones are used.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-mdeffn</code></strong>
</TD>
<TD VALIGN=TOP>
Sphinx3 format model definition file for continuous density models.
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-meanfn</code></strong><br>
<strong><code>-varfn</code></strong><br>
<strong><code>-mixwfn</code></strong>
</TD>
<TD VALIGN=TOP>
Sphinx3 format acoustic model files: means, variances and senone mixture weights.
(The transition matrices file has to be converted to Sphinx2 format and specified
via the <strong><code>-hmmdirlist</code></strong> argument.)
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-varfloor</code></strong>
</TD>
<TD VALIGN=TOP>
Floor for variance values in the <strong><code>-varfn</code></strong> file.
Smaller variance values are raised to this floor.
</TD>
<TD VALIGN=TOP>
0.0001
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-mixwfloor</code></strong>
</TD>
<TD VALIGN=TOP>
Floor for mixture weight values in the <strong><code>-mixwfn</code></strong> file.
Smaller values are raised to this floor.
</TD>
<TD VALIGN=TOP>
0.0000001
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-phonetpfn</code></strong>
</TD>
<TD VALIGN=TOP>
Phone transition probability (actually counts) matrix input file, for use
as a "language model" in allphone recognition mode.  (Also see associated
arguments <strong><code>-ptplw</code></strong> and
<strong><code>-uptpwt</code></strong>.)
</TD>
<TD VALIGN=TOP>
None.
</TD>
</TR>

</TABLE>


<br><br>

<H3><A NAME="sec_cmdline_searchconfig">Decoder Configuration</A></H3>

<TABLE border>

<TR>
<TH>Flag</TH>
<TH>Description</TH>
<TH>Default</TH>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-ctlfn</code></strong><br>
<strong><code>-ctloffset</code></strong><br>
<strong><code>-ctlcount</code></strong>
</TD>
<TD VALIGN=TOP>
Batch-mode control file listing utterance files (without their file-extension)
to decode.  <strong><code>-ctloffset</code></strong> is the number of initial
utterances in the file to be skipped, and <strong><code>-ctlcount</code></strong>
the number to be processed (after the skip, if any).
<strong><code>-ctlfn</code></strong> must not be specified for live-mode or
application-driven operation.
</TD>
<TD VALIGN=TOP>
None<br>
0<br>
All<br>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-datadir</code></strong><br>
</TD>
<TD VALIGN=TOP>
If the control file (-ctlfn argument) entries are relative pathnames, an optional
directory prefix for them may be specified using this argument.
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-allphone</code></strong><br>
</TD>
<TD VALIGN=TOP>
Should be <strong><code>TRUE</code></strong> to configure the recognition engine for
allphone mode operation.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-tactlfn</code></strong><br>
</TD>
<TD VALIGN=TOP>
Input transcript file, parallel to the control file (<strong><code>-ctlfn</code></strong>)
in forced alignment mode.
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-adcin</code></strong><br>
<strong><code>-adcext</code></strong><br>
<strong><code>-adchdr</code></strong><br>
<strong><code>-adcendian</code></strong>
</TD>
<TD VALIGN=TOP>
In batch mode, <strong><code>-adcin</code></strong> selects A/D
(<strong><code>TRUE</code></strong>) or cepstrum input data
(<strong><code>FALSE</code></strong>).
If <strong><code>TRUE</code></strong>, <strong><code>-adcext</code></strong>
is the file extension to be appended to names listed in the
<strong><code>-ctlfn</code></strong> argument file,
<strong><code>-adchdr</code></strong> the number of bytes of header in each
input file, and <strong><code>-adcendian</code></strong> their byte ordering:
0 for big-endian, 1 for little-endian.  With these flags, most A/D data file
formats can be processed directly.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong><br>
<strong><code>raw</code></strong><br>
0<br>
1
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-normmean</code></strong><br>
<strong><code>-nmprior</code></strong>
</TD>
<TD VALIGN=TOP>
Cepstral mean normalization (CMN) option.  If <strong><code>-nmprior</code></strong>
is <strong><code>FALSE</code></strong>, CMN computed on current utterance only
(usually batch mode), otherwise based on past history (live mode).
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong><br>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-compress</code></strong><br>
<strong><code>-compressprior</code></strong>
</TD>
<TD VALIGN=TOP>
Silence deletion (within decoder, not related to <A HREF="#sec_cont_ad">continuous
listening</A>).  If <strong><code>-compressprior</code></strong> is
<strong><code>FALSE</code></strong>, based on current utterance statistics
(batch mode), otherwise based on past history (live mode).
<strong><code>-compress</code></strong> should be
<strong><code>FALSE</code></strong> if continuous listening is used.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong><br>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-agcmax</code></strong><br>
<strong><code>-agcemax</code></strong>
</TD>
<TD VALIGN=TOP>
Automatic gain control (AGC) option.  In batch mode <em>only</em>
<strong><code>-agcmax</code></strong> should be <strong><code>TRUE</code></strong>,
and in live mode <em>only</em> <strong><code>-agcemax</code></strong>.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong><br>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-live</code></strong>
</TD>
<TD VALIGN=TOP>
Forces some live mode flags: <strong><code>-nmprior</code></strong>
<strong><code>-compressprior</code></strong> and
<strong><code>-agcemax</code></strong> to <strong><code>TRUE</code></strong>
if any AGC is on.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-samp</code></strong>
</TD>
<TD VALIGN=TOP>
Sampling rate; must be 8000 or 16000.
</TD>
<TD VALIGN=TOP>
<strong><code>16000</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fwdflat</code></strong>
</TD>
<TD VALIGN=TOP>
Run flat-lexical Viterbi search after tree-structured pass (for better
accuracy).  Usually <strong><code>FALSE</code></strong> in live mode.
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-bestpath</code></strong>
</TD>
<TD VALIGN=TOP>
Run global best path search over Viterbi search word lattice output (for better
accuracy).
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-compallsen</code></strong>
</TD>
<TD VALIGN=TOP>
Compute all senones, whether active or inactive, in each frame.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-latsize</code></strong>
</TD>
<TD VALIGN=TOP>
Word lattice entries to be allocated.  Longer sentences need larger lattices.
</TD>
<TD VALIGN=TOP>
50000
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fsgbfs</code></strong>
</TD>
<TD VALIGN=TOP>
If <strong><code>FALSE</code></strong>, backtrace from state with best path score,
instead of from FSG final state.  (FSG mode only.)
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong>
</TD>
</TR>

</TABLE>


<br><br>

<H3><A NAME="sec_cmdline_beam">Beam Widths</A></H3>

<TABLE border>

<TR>
<TH>Flag</TH>
<TH>Description</TH>
<TH>Default</TH>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-top</code></strong>
</TD>
<TD VALIGN=TOP>
Number of codewords computed per frame.  Usually, narrowed to 1 in live mode.
</TD>
<TD VALIGN=TOP>
4
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-beam</code></strong><br>
<strong><code>-npbeam</code></strong>
</TD>
<TD VALIGN=TOP>
Main pruning thresholds for tree search.  Usually narrowed down to 2e-6 in live mode.
</TD>
<TD VALIGN=TOP>
1e-6<br>
1e-6
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-lpbeam</code></strong>
</TD>
<TD VALIGN=TOP>
Additional pruning threshold for <em>transitions to</em> leaf nodes of lexical tree.
Usually narrowed down to 2e-5 in live mode.
</TD>
<TD VALIGN=TOP>
1e-5
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-lponlybeam</code></strong><br>
<strong><code>-nwbeam</code></strong>
</TD>
<TD VALIGN=TOP>
Yet more pruning thresholds for leaf nodes and exits from lexical tree.
Usually narrowed down to 5e-4 in live mode.
</TD>
<TD VALIGN=TOP>
3e-4<br>
3e-4
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-maxwpf</code></strong><br>
</TD>
<TD VALIGN=TOP>
Maximum number of words, ranked according to score, that can be recognized and entered
in the Viterbi history in each frame.  Essentially, an absolute pruning parameter
complementing the beam pruning parameter.  (Not used in FSG mode.)
</TD>
<TD VALIGN=TOP>
Infinity
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-maxhmmpf</code></strong><br>
</TD>
<TD VALIGN=TOP>
Absolute pruning threshold.  Maximum number of HMMs to keep active in each frame
(approx.).  Implemented only in FSG mode.
</TD>
<TD VALIGN=TOP>
Infinity
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-fwdflatbeam</code></strong><br>
<strong><code>-fwdflatnwbeam</code></strong>
</TD>
<TD VALIGN=TOP>
Main and word-exit pruning thresholds for the optional, flat lexical Viterbi search.
</TD>
<TD VALIGN=TOP>
1e-8<br>
3e-4
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-topsenfrm</code></strong><br>
<strong><code>-topsenthresh</code></strong>
</TD>
<TD VALIGN=TOP>
No. of lookahead frames for predicting active base phones.  (If &lt;=1, all base phones
assumed to be active every frame.)  <strong><code>-topsenthresh</code></strong> is
log(pruning threshold) applied to raw senone scores to determine active phones in each
frame.
</TD>
<TD VALIGN=TOP>
1<br>
-60000
</TD>
</TR>

</TABLE>


<br><br>

<H3><A NAME="sec_cmdline_langwt">Language Weights/Penalties</A></H3>

<TABLE border>

<TR>
<TH>Flag</TH>
<TH>Description</TH>
<TH>Default</TH>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-langwt</code></strong><br>
<strong><code>-fwdflatlw</code></strong><br>
<strong><code>-rescorelw</code></strong>
</TD>
<TD VALIGN=TOP>
Language weights applied during lexical tree Viterbi search, flat-structured Viterbi
search, and global word lattice search, respectively.
</TD>
<TD VALIGN=TOP>
6.5<br>
8.5<br>
9.5
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-ugwt</code></strong>
</TD>
<TD VALIGN=TOP>
Unigram weight for interpolating unigram probabilities with uniform distribution.
Typically in the range 0.5-0.8.
</TD>
<TD VALIGN=TOP>
1.0
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-inspen</code></strong><br>
<strong><code>-silpen</code></strong><br>
<strong><code>-fillpen</code></strong>
</TD>
<TD VALIGN=TOP>
Word insertion penalty or probability (for words in the LM),
insertion penalty for the silence word, and
insertion penalty for noise words (from <strong><code>-ndictfn</code></strong> file) if any.
</TD>
<TD VALIGN=TOP>
0.65<br>
0.005<br>
1e-8
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-oovugprob</code></strong>
</TD>
<TD VALIGN=TOP>
Unigram probability (logprob) for OOV words from
<strong><code>-oovdictfn</code></strong> file, if any.
</TD>
<TD VALIGN=TOP>
-4.5
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-ascrscale</code></strong>
</TD>
<TD VALIGN=TOP>
Scaling of acoustic scores (continuous density acoustic models only).  Raw acoustic
scores are first scaled down (i.e., shifted down) by so many bits.
</TD>
<TD VALIGN=TOP>
0 (no scaling)
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-ptplw</code></strong>
</TD>
<TD VALIGN=TOP>
Phone transition language weight applied to phone transition
probability matrix (see <strong><code>-phonetpfn</code></strong> argument).
</TD>
<TD VALIGN=TOP>
5.0
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-uptpwt</code></strong>
</TD>
<TD VALIGN=TOP>
Linear interpolation constant, for interpolating phone transition probabilities
between uniform and those specified by the
<strong><code>-phonetpfn</code></strong> argument.  Values closer to 1.0 weight
uniform distribution more, closer to 0.0 weight it less.
</TD>
<TD VALIGN=TOP>
0.001
</TD>
</TR>

</TABLE>


<br><br>

<H3><A NAME="sec_cmdline_outputspec">Output Specifications</A></H3>

<TABLE border>

<TR>
<TH>Flag</TH>
<TH>Description</TH>
<TH>Default</TH>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-matchfn</code></strong>
</TD>
<TD VALIGN=TOP>
Filename to which final recognition string for each utterance written.
(Old format, word-id at the end.)
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-matchsegfn</code></strong>
</TD>
<TD VALIGN=TOP>
Like <strong><code>-matchfn</code></strong>, but contains word segmentation
info: <em>startframe #frames word</em>...
(New format, word-id at the beginning.)
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-partial</code></strong>
</TD>
<TD VALIGN=TOP>
If greater than 0, print any available partial hypothesis to stdout every so
many frames.
</TD>
<TD VALIGN=TOP>
0
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-partialseg</code></strong>
</TD>
<TD VALIGN=TOP>
Like <strong><code>-partial</code></strong>, but include segmentation
information for each word in the partial hypothesis.
</TD>
<TD VALIGN=TOP>
0
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-reportpron</code></strong>
</TD>
<TD VALIGN=TOP>
Causes word pronunciation to be included in output files.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-rawlogdir</code></strong>
</TD>
<TD VALIGN=TOP>
If specified, logs raw A/D input samples for each utterance to the indicated directory.
(One file per utterance, named &lt;uttid&gt;.raw.)
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-mfclogdir</code></strong>
</TD>
<TD VALIGN=TOP>
If specified, logs cepstrum data for each utterance to the indicated directory.
(One file per utterance, named &lt;uttid&gt;.mfc.)
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-dumplatdir</code></strong>
</TD>
<TD VALIGN=TOP>
If specified, dumps word lattice for each utterance to a file in this directory.
The filename is created from the utterance ID.
</TD>
<TD VALIGN=TOP>
None
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-logfn</code></strong>
</TD>
<TD VALIGN=TOP>
Filename to which decoder logging information is written.
</TD>
<TD VALIGN=TOP>
stdout/stderr
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-backtrace</code></strong>
</TD>
<TD VALIGN=TOP>
Includes detailed word backtrace information in log file.
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-nbest</code></strong>
</TD>
<TD VALIGN=TOP>
No. of N-best hypotheses to be produced.  Currently, this flag is only useful in batch
mode.  But an application can always directly invoke
<A HREF="#sec_fbs"><strong><code>search_get_alt</code></strong></A> to obtain them.
Also, the current implementation is lacking in some details (e.g., in returning
detailed scores).
</TD>
<TD VALIGN=TOP>
0
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-nbestdir</code></strong>
</TD>
<TD VALIGN=TOP>
Directory to which N-best files written (one/utterance).
</TD>
<TD VALIGN=TOP>
Current dir.
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-nbestseg</code></strong>
</TD>
<TD VALIGN=TOP>
If FALSE, nbest hypotheses files contain only the word sequence for each alternative.
But, if TRUE, each word also includes segmentation and confidence score information.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-phoneconf</code></strong>
</TD>
<TD VALIGN=TOP>
Writes a form of phoneme scores for the utterance to the logfile.
Mainly for diagnostics purposes.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-pscr2lat</code></strong>
</TD>
<TD VALIGN=TOP>
Write a phoneme lattice for the utterance to the logfile,
i.e., the top-scoring phonemes per frame.  Mainly for diagnostics purposes.
</TD>
<TD VALIGN=TOP>
<strong><code>FALSE</code></strong>
</TD>
</TR>

<TR>
<TD VALIGN=TOP>
<strong><code>-taword</code></strong><br>
<strong><code>-taphone</code></strong><br>
<strong><code>-tastate</code></strong>
</TD>
<TD VALIGN=TOP>
Whether word, phone, and state alignment output should be produced when running
in forced alignment mode.
</TD>
<TD VALIGN=TOP>
<strong><code>TRUE</code></strong><br>
<strong><code>TRUE</code></strong><br>
<strong><code>FALSE</code></strong>
</TD>
</TR>

</TABLE>

<P><br>

Finally, one of the arguments can be:
<strong><code>-argfile</code></strong> <em>filename</em>.
This causes additional arguments to be read in from the given
file.  Lines beginning with the <strong><code>'#'</code></strong>
character in this file are ignored.
Recursive <strong><code>-argfile</code></strong> specifications are not allowed.


<br><br>

<H3><A NAME="sec_cmdline_list">Alphabetical List of Arguments</A></H3>

<TABLE>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">8bsen</A>:</TD>
<TD VALIGN=TOP>Use 8-bit senone dump file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">adcendian</A>:</TD>
<TD VALIGN=TOP>A/D input file byte-ordering.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">adcext</A>:</TD>
<TD VALIGN=TOP>A/D input file extension.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">adchdr</A>:</TD>
<TD VALIGN=TOP>No. bytes of header in A/D input file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">adcin</A>:</TD>
<TD VALIGN=TOP>Input file contains A/D samples or cepstra (TRUE/FALSE).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">agcemax</A>:</TD>
<TD VALIGN=TOP>Compute AGC (max C0 normalized to 0; estimated, live mode).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">agcmax</A>:</TD>
<TD VALIGN=TOP>Compute AGC (max C0 normalized to 0 based on current utterance).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">argfile</A>:</TD>
<TD VALIGN=TOP>Arguments file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">ascrscale</A>:</TD>
<TD VALIGN=TOP>Acoustic scores scaling for continuous density models.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">backtrace</A>:</TD>
<TD VALIGN=TOP>Provide detailed backtrace in log file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">beam</A>:</TD>
<TD VALIGN=TOP>Main pruning beamwidth.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">bestpath</A>:</TD>
<TD VALIGN=TOP>Run global best path algorithm on word lattice.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">cbdir</A>:</TD>
<TD VALIGN=TOP>Directory containing semi-continuous acoustic model codebook files.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">compallsen</A>:</TD>
<TD VALIGN=TOP>Compute all senones.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">compress</A>:</TD>
<TD VALIGN=TOP>Remove silence frames (based on C0 statistics).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">compressprior</A>:</TD>
<TD VALIGN=TOP>Remove silence frames (based on C0 statistics from prior history).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">ctlcount</A>:</TD>
<TD VALIGN=TOP>No. of utterances to decode in batch mode.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">ctlfn</A>:</TD>
<TD VALIGN=TOP>Control file listing utterances to decode in batch mode.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">ctloffset</A>:</TD>
<TD VALIGN=TOP>No. of initial utterances to be skipped from control file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">datadir</A>:</TD>
<TD VALIGN=TOP>Directory prefix for control file entries.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">dictfn</A>:</TD>
<TD VALIGN=TOP>Main pronunciation dictionary.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">dumplatdir</A>:</TD>
<TD VALIGN=TOP>Directory for dumping word lattices.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">fillpen</A>:</TD>
<TD VALIGN=TOP>Noise word penalty (probability).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">fsgctlfn</A>:</TD>
<TD VALIGN=TOP>Control file listing several FSG files to be loaded at initialization.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">fsgfn</A>:</TD>
<TD VALIGN=TOP>Finite state grammar file to be loaded at initialization time.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">fsgusealtpron</A>:</TD>
<TD VALIGN=TOP>Consider alternative pronunciations for every FSG transition.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">fsgusefiller</A>:</TD>
<TD VALIGN=TOP>Add a filler self-loop transition at every FSG state.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">fwdflat</A>:</TD>
<TD VALIGN=TOP>Run flat-lexical Viterbi search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">fwdflatbeam</A>:</TD>
<TD VALIGN=TOP>Main beam width for flat search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">fwdflatlw</A>:</TD>
<TD VALIGN=TOP>Language weight for flat search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">fwdflatnwbeam</A>:</TD>
<TD VALIGN=TOP>Word-exit beam width for flat search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">hmmdir</A>:</TD>
<TD VALIGN=TOP>Directory containing semi-continuous senone mixture weights files.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">hmmdirlist</A>:</TD>
<TD VALIGN=TOP>Directory containing semi-continuous HMM transition matrices files.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">inspen</A>:</TD>
<TD VALIGN=TOP>Word insertion penalty (probability).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">kbdumpdir</A>:</TD>
<TD VALIGN=TOP>Directory containing LM dump files.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">langwt</A>:</TD>
<TD VALIGN=TOP>Language weight for lexical tree search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">latsize</A>:</TD>
<TD VALIGN=TOP>Size of word lattice to be allocated.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">live</A>:</TD>
<TD VALIGN=TOP>Live mode.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">lmctlfn</A>:</TD>
<TD VALIGN=TOP>Control file listing named language model files to be loaded at initialization.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">lmfn</A>:</TD>
<TD VALIGN=TOP>Unnamed language model file to load at initialization.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">logfn</A>:</TD>
<TD VALIGN=TOP>Output log file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">lpbeam</A>:</TD>
<TD VALIGN=TOP>Transition to last phone beam width.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">lponlybeam</A>:</TD>
<TD VALIGN=TOP>Last phone internal beam width.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">mapfn</A>:</TD>
<TD VALIGN=TOP>Senone mapping file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">matchfn</A>:</TD>
<TD VALIGN=TOP>Output match file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">matchsegfn</A>:</TD>
<TD VALIGN=TOP>Output match file with word segmentation.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">maxhmmpf</A>:</TD>
<TD VALIGN=TOP>Max HMMs to keep active per frame (absolute pruning, approx.).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">maxwpf</A>:</TD>
<TD VALIGN=TOP>Max words to be recognized per frame (absolute pruning).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">mdeffn</A>:</TD>
<TD VALIGN=TOP>Model definition file for continuous density acoustic model.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">meanfn</A>:</TD>
<TD VALIGN=TOP>Continuous density acoustic model Gaussian means file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">mfclogdir</A>:</TD>
<TD VALIGN=TOP>Directory for logging cepstrum data for each utterance.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">mixwfloor</A>:</TD>
<TD VALIGN=TOP>Floor value for continuous density senone mixture weights.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">mixwfn</A>:</TD>
<TD VALIGN=TOP>Continuous density acoustic model senone mixture weights file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">nbest</A>:</TD>
<TD VALIGN=TOP>No. of N-best hypotheses to be produced/utterance.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">nbestdir</A>:</TD>
<TD VALIGN=TOP>Directory for writing N-best hypotheses files.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">nbestseg</A>:</TD>
<TD VALIGN=TOP>Whether to include segmentation for each nbest hypothesis word</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">ndictfn</A>:</TD>
<TD VALIGN=TOP>Noise words dictionary.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">nmprior</A>:</TD>
<TD VALIGN=TOP>Cepstral mean normalization based on prior utterances statistics.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">normmean</A>:</TD>
<TD VALIGN=TOP>Cepstral mean normalization.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">npbeam</A>:</TD>
<TD VALIGN=TOP>Next phone beam width for tree search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">nwbeam</A>:</TD>
<TD VALIGN=TOP>Word-exit beam width for tree search. </TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">oovdictfn</A>:</TD>
<TD VALIGN=TOP>Out-of-vocabulary words pronunciation dictionary.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">oovugprob</A>:</TD>
<TD VALIGN=TOP>Unigram probability for OOV words.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">partial</A>:</TD>
<TD VALIGN=TOP>Frequency of partial hypothesis reporting.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">partialseg</A>:</TD>
<TD VALIGN=TOP>Frequency of partial hypothesis (with segmentation) reporting.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">phnfn</A>:</TD>
<TD VALIGN=TOP>Phone file (senone mapping information).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">phoneconf</A>:</TD>
<TD VALIGN=TOP>Whether to generate phone segmentation and scores.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">pscr2lat</A>:</TD>
<TD VALIGN=TOP>Whether to output a phone lattice.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">rawlogdir</A>:</TD>
<TD VALIGN=TOP>Directory for logging A/D data for each utterance.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">reportpron</A>:</TD>
<TD VALIGN=TOP>Show actual word pronunciation in output match files.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">rescorelw</A>:</TD>
<TD VALIGN=TOP>Language weight for best path search.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">samp</A>:</TD>
<TD VALIGN=TOP>Input audio sampling rate(16000/8000).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">sendumpfn</A>:</TD>
<TD VALIGN=TOP>(8-bit) Senone dump file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">silpen</A>:</TD>
<TD VALIGN=TOP>Silence word penalty (probability).</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_searchconfig">tactlfn</A>:</TD>
<TD VALIGN=TOP>Forced alignment transcript file.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">taphone</A>:</TD>
<TD VALIGN=TOP>Whether phone-level alignment information should be output.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">tastate</A>:</TD>
<TD VALIGN=TOP>Whether state-level alignment information should be output.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_outputspec">taword</A>:</TD>
<TD VALIGN=TOP>Whether word-level alignment information should be output.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">top</A>:</TD>
<TD VALIGN=TOP>No. of top codewords to evaluate in each frame.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">topsenfrm</A>:</TD>
<TD VALIGN=TOP>No. of frames to lookahead to determine active base phones.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_beam">topsenthresh</A>:</TD>
<TD VALIGN=TOP>Pruning threshold applied to determine active base phones.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_langwt">ugwt</A>:</TD>
<TD VALIGN=TOP>Unigram weight for interpolating unigram probability with uniform probability.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">varfloor</A>:</TD>
<TD VALIGN=TOP>Floor value for continuous density Gaussian variance values.</TD>
</TR>
<TR>
<TD nowrap VALIGN=TOP><LI><A HREF="#sec_cmdline_inputdb">varfn</A>:</TD>
<TD VALIGN=TOP>Continuous density acoustic model Gaussian variances file.</TD>
</TR>
</TABLE>

<P>


<br><br><br><br><br><br>


<H2><A NAME="sec_faq"><U>Frequently Asked Questions</U></A></H2>

A fair amount of the available functionality in Sphinx2 hasn't been documented above.
In addition to the topics covered below, the reader is encouraged to look at
<strong><code>include/fbs.h</code></strong> carefully, in order to find out more.


<br><br>

<H3><A NAME="sec_tune">Decoder Tuning</A></H3>

There are several ways to speed up decoding:
<UL>
<LI> Tightening pruning thresholds:  Increasing
<A HREF="#sec_cmdline_beam"><strong><code>-beam</code></strong></A>
<A HREF="#sec_cmdline_beam"><strong><code>-npbeam</code></strong></A>
<A HREF="#sec_cmdline_beam"><strong><code>-lpbeam</code></strong></A>
<A HREF="#sec_cmdline_beam"><strong><code>-lponlybeam</code></strong></A>,
and
<A HREF="#sec_cmdline_beam"><strong><code>-nwbeam</code></strong></A>
uniformly by a factor &gt;1.

<LI> Setting absolute pruning thresholds using
 <A HREF="#sec_cmdline_beam"><strong><code>-maxwpf</code></strong></A>
(Ngram mode), or
 <A HREF="#sec_cmdline_beam"><strong><code>-maxhmmpf</code></strong></A>
(FSG mode).  Statistics
on the number of active HMMs and words hypothesized are written to the log file
(the <strong><code>-logfn</code></strong> argument), which can be examined to
estimate reasonable values for these parameters.

<LI> Reducing <A HREF="#sec_cmdline_beam"><strong><code>-top</code></strong></A>
from 4 to 2 or 1.

<LI> Using phone activation with
<A HREF="#sec_cmdline_beam"><strong><code>-topsenfrm</code></strong></A> &gt;1,
and adjusting the corresponding pruning beamwidth
<A HREF="#sec_cmdline_beam"><strong><code>-topsenthresh</code></strong></A>.
The former can be set, for example, to 4, and the latter between -60000 and -80000.
(Threshold values closer to 0 provide tigther pruning.)

<LI> Controlling
<A HREF="#sec_cmdline_searchconfig"><strong><code>-compallsen</code></strong></A>.
When <A HREF="#sec_cmdline_beam"><strong><code>-top</code></strong></A> is 1,
it is generally more efficient to compute <em>all</em> senones, but not when
<A HREF="#sec_cmdline_beam"><strong><code>-top</code></strong></A> is 4.  However,
when using very small vocabularies of just tens of words, it is preferable to
compute only the active senones, regardless of the value of
<A HREF="#sec_cmdline_beam"><strong><code>-top</code></strong></A>.  (But if
<A HREF="#sec_cmdline_beam"><strong><code>-topsenfrm</code></strong></A> &gt;1,
all senones are computed anyway.)

<LI> Using acoustic models with fewer senones.  (The Sphinx3 trainer can be used
to build such new models.)

<LI> Switching to a context specific language model to restrict the active
vocabulary for each utterance.  (Remember that the active vocabulary is the
<em>intersection</em> of the currently active LM and the dictionary.)
</UL>

<P>

When using continuous density models, the tuning parameters can be quite different
from those for semi-continuous models.  In particular, the dynamic range of
acoustic scores is usually larger, hence the language-weight
(<strong><code>-langwt</code></strong> argument) probably needs to be increased.
All the pruning thresholds (<strong><code>-beam</code></strong>,
<strong><code>-npbeam</code></strong>, <strong><code>-nwbeam</code></strong>,
<strong><code>-lpbeam</code></strong>, <strong><code>-lponlybeam</code></strong>,
<strong><code>-fwdflatbeam</code></strong> and
<strong><code>-fwdflatnwbeam</code></strong>) have to be lowered as well.
It is preferable to disable <em>phone lookahead</em> (i.e., not specifying the
<strong><code>-topsen</code></strong> argument), since phone lookahead requires
the evaluation of all senones every frame, which is expensive for continuous
density models.

<P>

Finally, to reduce the problem of integer overflow caused by the larger dynamic
range of scores, an acoustic scaling parameter
(<strong><code>-ascrscale</code></strong> argument) has been added.  This
parameter specifies the number of bits by which raw acoustic likelihoods are
scaled down.  For instance, a value of 1 implies that raw acoustic scores are
halved, thus compressing the dynamic range.  (Unfortunately, this usually
requires further tuning of language-weight and pruning threshold parameters.)



<br><br>

<H3><A NAME="sec_lmdump">Building LM Dump Files</A></H3>

LM files are usually ASCII files.  If they are large, it is time consuming to read
them into the decoder.  A binary "dump" file is much faster to read and more compact.

<P>
LM dump files can be created by either a standalone program
<strong><code>examples/lm3g2dmp.c</code></strong>
or the decoder.  The standalone version can be compiled from the
<strong><code>examples</code></strong></A> directory.

The program takes two arguments, the LM source file and a directory in which the
dump file is to be created.  It reads the header from the original LM file to determine
the size of the LM.  It then forms the binary dump file name by appending a
<strong><code>.DMP</code></strong> extension to the LM file name.  This file
is written to the second (directory) argument.  (<strong>NOTE:</strong> The dump
file must not already exist!!)

<P>
Any version of the decoder can also automatically create binary "dump" files
similar to the standalone version described above.  It first looks for the
dump file in the directory given by the
<A HREF="#sec_cmdline_inputdb"><strong><code>-kbdumpdir</code></strong></A>
argument.  If the dump file is present it reads it and ignores the rest of the
original LM file.  Otherwise, it reads the LM file and creates a dump file in the
<A HREF="#sec_cmdline_inputdb"><strong><code>-kbdumpdir</code></strong></A>
directory so that it can be used in subsequent decoder runs.

<P>
The decoder does not create dump files for small LMs that have fewer than an
internally defined number of bigrams and trigrams.



<br><br>

<H3><A NAME="sec_sendump">Building 8-Bit Senone Dump Files</A></H3>

The Sphinx-II senonic acoustic model files contain 32-bit data.  (These are in the
directory specified by the
<A HREF="#sec_cmdline_inputdb"><strong><code>-hmmdir</code></strong></A>
argument.)  However, they can be
<em>clustered</em> down to 8-bits for memory efficiency, without loss of recognition
accuracy.  The clustering is carried out by an offline process as follows:
<OL>
<LI> Create a <em>temporary</em> 32-bit senone dump file by running the decoder with
the <strong><code>-sendumpfn</code></strong> flag set to the temporary file name,
the <strong><code>-8bsen</code></strong> flag set to
<strong><code>FALSE</code></strong>, and omitting the
<strong><code>-lmfn</code></strong> argument.
The decoder can be killed after it creates the 32-bit senone dump file, which happens
during the initialization and is announced in the log output.

<LI> Run: <strong><code>/afs/cs/project/plus-2/s2/Sphinx2/bin/alpha/pdf32to8b 32bit-file 8bit-file</code></strong><BR>
to create the 8-bit senone dump file.
That is, the first argument to <strong><code>pdf32to8b</code></strong>
is the temporary 32-bit dump file created above, and the second argument is the
8-bit output file.

<LI> Delete the temporary 32-bit file.
</OL>
The 8-bit senone dump file can now be used as the <strong><code>-sendumpfn</code></strong>
argument to the decoder with the <strong><code>-8bsen</code></strong> argument set to <strong><code>TRUE</code></strong>.



<br><br>

<H3><A NAME="sec_fsgfmt">Finite State Grammar File Format</A></H3>

The finite state grammar file format is quite rudimentary.  It can be thought
of as an "assembly language" level specification.  It is hoped that most, if
not all, existing formats can be compiled or pre-processed down to this level.
(<strong>Note:</strong> The file format may change in the future, depending on
feedback from users.)

<P>

An FSG file looks as follows:
<pre>
   FSG_BEGIN [&lt;fsgname&gt;]
   NUM_STATES &lt;#states&gt;
   START_STATE &lt;start-state&gt;
   FINAL_STATE &lt;final-state&gt;
   TRANSITION &lt;from-state&gt; &lt;to-state&gt; &lt;prob&gt; [&lt;word-string&gt;]
   TRANSITION &lt;from-state&gt; &lt;to-state&gt; &lt;prob&gt; [&lt;word-string&gt;]
   ... (any number of state transitions)
   FSG_END
</pre>

<P>

The FSG spec begins with the line containing the keyword FSG_BEGIN.  (All
preceding lines are comments.)
It may have an optional FSG name string.  If no name is present, the FSG
has the empty string as its name.
Following the FSG_BEGIN declaration is the number of states in the FSG,
the start state, and the final state, each on a separate line.
States are numbered in the range [0 .. &lt;#states&gt;-1].
These are then followed by all the state transitions, in no particular order,
and each transition on a separate line.  The FSG specification is ended
by the FSG_END line.
The keywords <strong><code>NUM_STATES</code></strong>,
<strong><code>START_STATE</code></strong>,
<strong><code>FINAL_STATE</code></strong>, and
<strong><code>TRANSITION</code></strong>, can be abbreviated to
 <strong><code>N</code></strong>,
<strong><code>S</code></strong>,
<strong><code>F</code></strong>, and
<strong><code>T</code></strong>, respectively.

<P>

A transition specifies the source state, the destination state, the prior
probability of the transition being taken, and it optionally emits a word.
If no word emission is specified, it is an <em>epsilon</em> or <em>null</em>
transition.  If a transition emits a word that is not in the pronunciation
lexicon, the transition is simply ignored.

<P>

Note that the decoder automatically adds filler word transitions (such as
the silence word) at every state (i.e., from a state to itself; assuming the
<strong><code>-fsgusefiller</code></strong> argument is
<strong><code>TRUE</code></strong>).  This is the principal mechanism for
transparently allowing optional silences to occur in between any two words
in any sentence allowed by the FSG.

<P>

Comment lines can also be embedded anywhere in the file.
Any line that begins with the <strong><code>#</code></strong> character is
treated as a comment line.  Blank lines are also allowed anywhere in the file.


<br><br>

<H3><A NAME="sec_ptpfmt">Phone Transition Probability File Format</A></H3>

This file, specified by the <strong><code>-phonetpfn</code></strong> argument,
is actually a <em>counts</em> file, indicating the frequency with which
transitions between any pair of phonemes take place.  This information can be
easily obtained from some suitable training text.  Each line in the file
specifies a source phoneme, a destination phoneme, and the associated count
for transitions from the former to the latter.  Comment lines are allowed,
indicated by a <strong><code>#</code></strong> character in the first column.



<br><br>

<hr>
<br>
<address><a href="mailto:rkm@cs.cmu.edu">Ravishankar Mosur</a></address>
<!-- Created: Jan 01, 1997 -->
<!-- hhmts start -->
Last modified: Tue Mar 21 16:44:00 EST 2006
<!-- hhmts end -->

</BODY>
</HTML>
